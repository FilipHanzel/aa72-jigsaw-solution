{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from itertools import chain, combinations, islice, product\n",
    "from typing import Any, TypeAlias\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import ray\n",
    "from sklearn.neighbors import KDTree\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(\"aa72\")\n",
    "with open(\"logging.config.json\", \"rt\") as f:\n",
    "    logging.config.dictConfig(json.load(f))\n",
    "\n",
    "Point: TypeAlias = npt.NDArray[np.floating]\n",
    "PointArray: TypeAlias = npt.NDArray[np.floating]\n",
    "\n",
    "Side: TypeAlias = dict[str, str | PointArray]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 39106.09it/s]\n"
     ]
    }
   ],
   "source": [
    "IN_PATH = os.path.join(\"data\", \"out_02\", \"out.pickle\")\n",
    "\n",
    "sides: dict[int, list[Side]] = {}\n",
    "with open(IN_PATH, \"rb\") as f:\n",
    "    for piece_id, piece_sides in tqdm(pickle.load(f)):\n",
    "\n",
    "        if piece_id in range(40 + 1):\n",
    "            for piece_side in piece_sides:\n",
    "                piece_side[\"points\"] = np.array(piece_side[\"points\"])\n",
    "\n",
    "            sides[piece_id] = piece_sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side comparison score\n",
    "\n",
    "Calculate the comparison score between two sets of points using the Iterative Closest Point (ICP) algorithm.\n",
    "The comparison score represents the average distance between corresponding points in the source and destination sets. Lower score means better match.\n",
    "\n",
    "The order of the source and destination arrays may affect the result.\n",
    "\n",
    "*Based on [this tutorial](https://nghiaho.com/?page_id=671) and [this icp implementation](https://github.com/ClayFlannigan/icp).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icp(src: PointArray, dst: PointArray) -> float:\n",
    "    src_rows, src_dims = src.shape\n",
    "    dst_rows, dst_dims = dst.shape\n",
    "\n",
    "    # Should work for points with any dimensions\n",
    "    n = max(src_dims, dst_dims)\n",
    "\n",
    "    # Make points homogenious (this also ensures the same dimensionality for src and dst)\n",
    "    src_h = np.zeros((src_rows, n + 1), dtype=np.float32)\n",
    "    dst_h = np.zeros((dst_rows, n + 1), dtype=np.float32)\n",
    "    src_h[:, n] = 1\n",
    "    dst_h[:, n] = 1\n",
    "    src_h[:, :src_dims] = src\n",
    "    dst_h[:, :dst_dims] = dst\n",
    "\n",
    "    prev_error = np.inf  # error history for early stop\n",
    "    score = np.inf  # score (average error)\n",
    "\n",
    "    tree = KDTree(dst_h, leaf_size=5, p=1)\n",
    "\n",
    "    iterations = 1000\n",
    "    for _ in range(iterations):\n",
    "        # Guess which points in dst correspond to points in src based on shortest distance\n",
    "        distances, idxs = tree.query(src_h, k=1)\n",
    "\n",
    "        # Calculate error as average distance between points in dst and src\n",
    "        error = np.mean(distances)\n",
    "\n",
    "        # Update the score\n",
    "        score = error if error < score else score\n",
    "\n",
    "        # Early stop if there is no improvement or error gets worse\n",
    "        if np.abs(prev_error - error) < 10e-6:\n",
    "            break\n",
    "        prev_error = error\n",
    "\n",
    "        # Select subset of points from dst for transformations\n",
    "        dst_s = dst_h[idxs.flatten()]\n",
    "\n",
    "        # Translate to centroids\n",
    "        src_centroid = np.mean(src_h, axis=0)\n",
    "        dst_centroid = np.mean(dst_s, axis=0)\n",
    "        src_c = src_h - src_centroid\n",
    "        dst_c = dst_s - dst_centroid\n",
    "\n",
    "        # Calculate rotation matrix\n",
    "        h = src_c.T @ dst_c\n",
    "        u, _, v = np.linalg.svd(h)\n",
    "        r = v.T @ u.T\n",
    "\n",
    "        # Special reflection case\n",
    "        if np.linalg.det(r) < 0:\n",
    "            v[-1, :] *= -1\n",
    "            r = v.T @ u.T\n",
    "\n",
    "        # Translation vector\n",
    "        t = dst_centroid - r @ src_centroid.T\n",
    "\n",
    "        # Apply transformations to src\n",
    "        src_h = (r @ src_h.T).T + t\n",
    "    else:\n",
    "        # Final score update in case of no early stop\n",
    "        distances, idxs = tree.query(src_h, k=1)\n",
    "        error = np.mean(distances)\n",
    "        score = error if error < score else score\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.75 0.3278737850487232\n",
      "score: 1.25 0.608783908188343\n",
      "score: 0.25 0.25\n",
      "score: 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "a = np.array([[0, 1], [1, 1], [2, 1], [3, 2]], dtype=float)\n",
    "b = np.array([[1, 1], [2, 2], [2, 3], [2, 4]], dtype=float)\n",
    "print(\"score:\", icp(a, b), icp(b, a))\n",
    "\n",
    "a = np.array([[0, 1], [1, 1], [2, 1], [3, 2]], dtype=float)\n",
    "b = np.array([[2, -1], [2, 0], [2, 1], [1, 2]], dtype=float)\n",
    "print(\"score:\", icp(a, b), icp(b, a))\n",
    "\n",
    "a = np.array([[0, 1], [1, 1], [2, 1], [3, 1]], dtype=float)\n",
    "b = np.array([[1, 1], [2, 1], [3, 1], [4, 1]], dtype=float)\n",
    "print(\"score:\", icp(a, b), icp(b, a))\n",
    "\n",
    "a = np.array([[0, 1], [1, 1], [2, 1], [3, 1]], dtype=float)\n",
    "b = np.array([[0, 2], [1, 2], [2, 2], [3, 2]], dtype=float)\n",
    "print(\"score:\", icp(a, b), icp(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(\n",
    "    src_id: int,\n",
    "    src: PointArray,\n",
    "    dst_id: int,\n",
    "    dst: PointArray,\n",
    "    reverse=True,\n",
    ") -> float:\n",
    "    # Try matching larger side to smaller (include error caused by length difference)\n",
    "    if len(dst) > len(src):\n",
    "        dst, src = src, dst\n",
    "\n",
    "    # Ensure the output is deterministic(ish) in edge cases\n",
    "    elif len(dst) == len(src) and src_id > dst_id:\n",
    "        dst, src = src, dst\n",
    "\n",
    "    # If checking hole/knob match, one has to be reversed before ICP score\n",
    "    if reverse:\n",
    "        src = -src\n",
    "        src[:, 0] -= src[:, 0].min()\n",
    "        src[:, 1] -= src[:, 1].min()\n",
    "\n",
    "    # Subsample source, to speed up calculations\n",
    "    keep_percentage = 0.2\n",
    "    subsampling_step = int(1 / keep_percentage)\n",
    "\n",
    "    # Cut margin to avoid noise at the ends of the side\n",
    "    # TODO: Move this outside to avoid repeating\n",
    "    cut_margin = 0.02\n",
    "    src_y = src[:, 1]\n",
    "    dst_y = dst[:, 1]\n",
    "    src_cut_margin = abs(src_y.max() - src_y.min()) * cut_margin\n",
    "    dst_cut_margin = abs(dst_y.max() - dst_y.min()) * cut_margin\n",
    "    src = src[(src_y.max() - src_cut_margin > src_y) & (src_y > src_y.min() + src_cut_margin)]  # fmt: skip\n",
    "    dst = dst[(dst_y.max() - dst_cut_margin > dst_y) & (dst_y > dst_y.min() + dst_cut_margin)]  # fmt: skip\n",
    "\n",
    "    return icp(src[::subsampling_step], dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_idx = 0 (flat), b_idx = 0 (knob); score: 141.778\n",
      "a_idx = 0 (flat), b_idx = 1 (knob); score:  97.327\n",
      "a_idx = 0 (flat), b_idx = 2 (flat); score:  23.174\n",
      "a_idx = 0 (flat), b_idx = 3 (hole); score: 101.284\n",
      "a_idx = 1 (flat), b_idx = 0 (knob); score: 136.988\n",
      "a_idx = 1 (flat), b_idx = 1 (knob); score:  94.756\n",
      "a_idx = 1 (flat), b_idx = 2 (flat); score:   6.752\n",
      "a_idx = 1 (flat), b_idx = 3 (hole); score:  97.258\n",
      "a_idx = 2 (knob), b_idx = 0 (knob); score:  39.983\n",
      "a_idx = 2 (knob), b_idx = 1 (knob); score:  10.699\n",
      "a_idx = 2 (knob), b_idx = 2 (flat); score:  96.228\n",
      "a_idx = 2 (knob), b_idx = 3 (hole); score:  56.416\n",
      "a_idx = 3 (knob), b_idx = 0 (knob); score:  50.655\n",
      "a_idx = 3 (knob), b_idx = 1 (knob); score:  90.836\n",
      "a_idx = 3 (knob), b_idx = 2 (flat); score:  96.839\n",
      "a_idx = 3 (knob), b_idx = 3 (hole); score:   1.631\n"
     ]
    }
   ],
   "source": [
    "piece_a_id = 1\n",
    "piece_b_id = 2\n",
    "sides_a = sides[piece_a_id]\n",
    "sides_b = sides[piece_b_id]\n",
    "\n",
    "reverse = sides_a is not sides_b\n",
    "\n",
    "for a_idx, side_a in enumerate(sides_a):\n",
    "    for b_idx, side_b in enumerate(sides_b):\n",
    "        score = get_score(\n",
    "            piece_a_id,\n",
    "            side_a[\"points\"],\n",
    "            piece_b_id,\n",
    "            side_b[\"points\"],\n",
    "            reverse,\n",
    "        )\n",
    "        print(\n",
    "            f\"{a_idx = } ({side_a['type']}), {b_idx = } ({side_b['type']}); score: {score:7.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building comparison index\n",
    "\n",
    "Compare all knobs with all holes to build an index with match scores. Scores are precomputed and stored to avoid recalculating them while solving the jigsaw.\n",
    "\n",
    "Comparison index is a csv file with following columns: \\\n",
    "`piece_a_name`, `side_a_idx`, `side_a_type`, `piece_b_name`, `side_b_idx`, `side_b_type`, `score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(sides: dict[int, list[Side]], index_file_path: str) -> None:\n",
    "    if os.path.exists(index_file_path):\n",
    "        os.remove(index_file_path)\n",
    "    if not os.path.exists(os.path.dirname(index_file_path)):\n",
    "        os.makedirs(os.path.dirname(index_file_path))\n",
    "\n",
    "    log.info(\"Processing...\")\n",
    "\n",
    "    n_pieces = len(sides)\n",
    "    n_comparisons = 16 * (n_pieces - 1) * n_pieces // 2\n",
    "\n",
    "    with open(index_file_path, \"wt\") as f, tqdm(total=n_comparisons) as pbar:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for piece_a_id, piece_b_id in combinations(range(1, n_pieces + 1), 2):\n",
    "            sides_a = sides[piece_a_id]\n",
    "            sides_b = sides[piece_b_id]\n",
    "\n",
    "            for (side_a_idx, side_a), (side_b_idx, side_b) in product(\n",
    "                enumerate(sides_a), enumerate(sides_b)\n",
    "            ):\n",
    "                if (side_a[\"type\"], side_b[\"type\"]) in (\n",
    "                    (\"knob\", \"hole\"),\n",
    "                    (\"hole\", \"knob\"),\n",
    "                ):\n",
    "                    score = get_score(piece_a_id, side_a[\"points\"], piece_b_id, side_b[\"points\"], reverse=True)\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            piece_a_id, side_a_idx, side_a['type'],  # fmt: skip\n",
    "                            piece_b_id, side_b_idx, side_b['type'],  # fmt: skip\n",
    "                            score\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 07:44:46][INFO] Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12480/12480 [00:37<00:00, 337.10it/s]\n"
     ]
    }
   ],
   "source": [
    "build_index(sides, os.path.join(\"data\", \"out_03\", \"sample_index_sp.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def compare_pieces(\n",
    "    piece_a_sides: tuple[int, list[Side]],\n",
    "    piece_b_sides: tuple[int, list[Side]],\n",
    ") -> list[list[str | int]]:\n",
    "    piece_a_id, sides_a = piece_a_sides\n",
    "    piece_b_id, sides_b = piece_b_sides\n",
    "\n",
    "    result = []\n",
    "    for (side_a_idx, side_a), (side_b_idx, side_b) in product(\n",
    "        enumerate(sides_a), enumerate(sides_b)\n",
    "    ):\n",
    "        if (side_a[\"type\"], side_b[\"type\"]) in (\n",
    "            (\"knob\", \"hole\"),\n",
    "            (\"hole\", \"knob\"),\n",
    "        ):\n",
    "            result.append(\n",
    "                [\n",
    "                    # fmt: off\n",
    "                    piece_a_id, side_a_idx, side_a[\"type\"],\n",
    "                    piece_b_id, side_b_idx, side_b[\"type\"],\n",
    "                    get_score(piece_a_id, side_a[\"points\"], piece_b_id, side_b[\"points\"], reverse=True),\n",
    "                    # fmt: on\n",
    "                ]\n",
    "            )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index_ray(\n",
    "    sides: dict[int, list[Side]],\n",
    "    index_file_path: str,\n",
    "    clear_sides: bool = False,\n",
    ") -> None:\n",
    "    log.info(\"Filling up object store memory...\")\n",
    "    ray_data_ids = [ray.put(item) for item in sides.items()]\n",
    "\n",
    "    if clear_sides:\n",
    "        sides.clear()\n",
    "\n",
    "    log.info(\"Queueing tasks...\")\n",
    "    tasks = [\n",
    "        compare_pieces.remote(ray_id_piece_a, ray_id_piece_b)\n",
    "        for ray_id_piece_a, ray_id_piece_b in combinations(ray_data_ids, 2)\n",
    "    ]\n",
    "\n",
    "    def yield_ray(tasks: list[ray.ObjectRef]) -> Any:\n",
    "        while tasks:\n",
    "            done, tasks = ray.wait(tasks)\n",
    "            yield ray.get(done[0])\n",
    "\n",
    "    log.info(\"Yielding tasks...\")\n",
    "    with open(index_file_path, \"wt\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for result in tqdm(yield_ray(tasks), total=len(tasks)):\n",
    "            writer.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 07:45:38,578\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 07:45:39][INFO] Filling up object store memory...\n",
      "[2024-02-06 07:45:39][INFO] Queueing tasks...\n",
      "[2024-02-06 07:45:39][INFO] Yielding tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:05<00:00, 140.10it/s]\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "build_index_ray(sides, os.path.join(\"data\", \"out_03\", \"sample_index_ray.csv\"))\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed with queue limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combinations_count(iterable: collections.Sized, r: int) -> int:\n",
    "    n = len(iterable)\n",
    "    return math.factorial(n) // (math.factorial(r) * math.factorial(n - r))\n",
    "\n",
    "\n",
    "def build_index_ray_limited(\n",
    "    sides: dict[int, list[Side]],\n",
    "    index_file_path: str,\n",
    "    queue_limit: int,\n",
    "    clear_sides: bool = False,\n",
    ") -> None:\n",
    "    log.info(\"Filling up object store memory...\")\n",
    "    ray_data_ids = [ray.put(item) for item in sides.items()]\n",
    "\n",
    "    if clear_sides:\n",
    "        sides.clear()\n",
    "\n",
    "    log.info(\"Starting limited queue loop...\")\n",
    "    with (\n",
    "        open(index_file_path, \"wt\") as f,\n",
    "        tqdm(total=_combinations_count(ray_data_ids, 2)) as pbar,\n",
    "    ):\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        tasks = []\n",
    "        it = combinations(ray_data_ids, 2)\n",
    "        while True:\n",
    "            done, tasks = ray.wait(\n",
    "                tasks,\n",
    "                num_returns=min(queue_limit, len(tasks)),\n",
    "                timeout=1,\n",
    "            )\n",
    "\n",
    "            to_fill = list(islice(it, queue_limit - len(tasks)))\n",
    "            if not to_fill:\n",
    "                break\n",
    "\n",
    "            for a, b in to_fill:\n",
    "                tasks.append(compare_pieces.remote(a, b))\n",
    "\n",
    "            for record in done:\n",
    "                writer.writerows(ray.get(record))\n",
    "                pbar.update()\n",
    "\n",
    "        for record in chain(done, tasks):\n",
    "            writer.writerows(ray.get(record))\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 07:46:51,645\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 07:46:52][INFO] Filling up object store memory...\n",
      "[2024-02-06 07:46:52][INFO] Starting limited queue loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:06<00:00, 118.50it/s]\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "build_index_ray_limited(\n",
    "    sides,\n",
    "    os.path.join(\"data\", \"out_03\", \"sample_index_ray_limited.csv\"),\n",
    "    queue_limit=100,\n",
    ")\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_regex = re.compile(r\"(\\d+,\\d+,\\w+),(\\d+,\\d+,\\w+),(\\d+.\\d+)\")\n",
    "\n",
    "with (\n",
    "    open(os.path.join(\"data\", \"out_03\", \"sample_index_sp.csv\"), \"rt\") as index_sp,\n",
    "    open(os.path.join(\"data\", \"out_03\", \"sample_index_ray.csv\"), \"rt\") as index_ray,\n",
    "    open(os.path.join(\"data\", \"out_03\", \"sample_index_ray_limited.csv\"), \"rt\") as index_ray_limited,  # fmt: skip\n",
    "):\n",
    "    index_sp = set(\n",
    "        frozenset(re.match(record_regex, record).groups())\n",
    "        for record in index_sp.read().strip().split(\"\\n\")\n",
    "    )\n",
    "    index_ray = set(\n",
    "        frozenset(re.match(record_regex, record).groups())\n",
    "        for record in index_ray.read().strip().split(\"\\n\")\n",
    "    )\n",
    "    index_ray_limited = set(\n",
    "        frozenset(re.match(record_regex, record).groups())\n",
    "        for record in index_ray_limited.read().strip().split(\"\\n\")\n",
    "    )\n",
    "\n",
    "    assert index_sp == index_ray == index_ray_limited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1052.68it/s]\n",
      "2024-02-06 07:48:08,182\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-06 07:48:08][INFO] Filling up object store memory...\n",
      "[2024-02-06 07:48:09][INFO] Starting limited queue loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999000/1999000 [5:27:14<00:00, 101.81it/s]  \n"
     ]
    }
   ],
   "source": [
    "sides: dict[int, list[Side]] = {}\n",
    "with open(IN_PATH, \"rb\") as f:\n",
    "    for piece_id, piece_sides in tqdm(pickle.load(f)):\n",
    "        for piece_side in piece_sides:\n",
    "            piece_side[\"points\"] = np.array(piece_side[\"points\"])\n",
    "        sides[piece_id] = piece_sides\n",
    "\n",
    "ray.init()\n",
    "build_index_ray_limited(\n",
    "    sides,\n",
    "    os.path.join(\"data\", \"out_03\", \"index.csv\"),\n",
    "    queue_limit=10_000,\n",
    ")\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
